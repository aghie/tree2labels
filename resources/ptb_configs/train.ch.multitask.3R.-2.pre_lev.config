### use # to comment out the configure item
### I/O ###

train_dir=sample_data/cp_datasets/ptb/ptb-train.multitask.3R.-2.pre_lev
dev_dir=sample_data/cp_datasets/ptb/ptb-dev.multitask.3R.-2.pre_lev
test_dir=sample_data/cp_datasets/ptb/ptb-dev.multitask.3R.-2.pre_lev

model_dir=parsing_models/ptb/ptb.multitask.3R.-2.pre_lev.2
word_emb_dir=/home/david.vilares/Escritorio/glove.6B.100d.txt

#pretrained_model=/tmp/pretrained-proof.model
##[all|lstms]
#pretrained_part=lstms

#raw_dir=
#decode_dir=
#dset_dir=
#load_model_dir=
#char_emb_dir=


norm_word_emb=False
norm_char_emb=False
number_normalized=False
seg=False
word_emb_dim=100
char_emb_dim=30

###NetworkConfiguration###
use_crf=False
use_char=True
word_seq_feature=LSTM
char_seq_feature=LSTM
feature=[POS] emb_size=20


###TrainingSetting###
status=train
optimizer=SGD
iteration=100
batch_size=8
ave_batch_loss=True

###Hyperparameters###
cnn_layer=4
char_hidden_dim=50
hidden_dim=800
dropout=0.5
lstm_layer=2
bilstm=True
learning_rate=0.02
lr_decay=0.05
momentum=0.9
l2=0
gpu=False
#clip=
main_tasks=3
tasks=4
tasks_weights=1|1|1|0.1
optimize_with_evalb=True


###PathsToAdditionalScripts###
tree2labels=tree2labels
en2mt=tree2labels/encoding2multitask.py
evaluate=tree2labels/evaluate.py
evalb=tree2labels/EVALB/evalb
gold_dev_trees=sample_data/cp_datasets/PTB_pred_tags/dev.trees





