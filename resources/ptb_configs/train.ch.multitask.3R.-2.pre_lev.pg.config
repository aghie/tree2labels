### use # to comment out the configure item
### I/O ###

train_dir=../sample_data/cp_datasets/ptb/ptb-train.multitask.3R.-2.pre_lev
dev_dir=../sample_data/cp_datasets/ptb/ptb-dev.multitask.3R.-2.pre_lev
test_dir=../sample_data/cp_datasets/ptb/ptb-dev.multitask.3R.-2.pre_lev

model_dir=multitask.3R.-2.pre_lev.pg.sample30.lr0005.varred.norm5.entropy002
word_emb_dir=resources/glove.6B.100d.txt

pretrained_model=../parsing_models/ptb.multitask.3R.-2.pre_lev.model

#raw_dir=
#decode_dir=
#dset_dir=
#load_model_dir=
#char_emb_dir=


norm_word_emb=False
norm_char_emb=False
number_normalized=False
seg=False
word_emb_dim=100
char_emb_dim=30

###NetworkConfiguration###
use_crf=False
use_char=False
word_seq_feature=LSTM
char_seq_feature=LSTM
feature=[POS] emb_size=20


###TrainingSetting###
status=finetune
optimizer=SGD
iteration=5
batch_size=1
ave_batch_loss=True

###Hyperparameters###
cnn_layer=4
char_hidden_dim=50
hidden_dim=800
dropout=0.5
lstm_layer=2
bilstm=True
learning_rate=0.0005
lr_decay=0.05
momentum=0.9
l2=0
gpu=True
#clip=
main_tasks=3
tasks=4
tasks_weights=1|1|1|0.1
optimize_with_evalb=True


###PathsToAdditionalScripts###
tree2labels=tree2labels
en2mt=tree2labels/encoding2multitask.py
evaluate=tree2labels/evaluate.py
evalb=EVALB/evalb
gold_dev_trees=../sample_data/cp_datasets/PTB_pred_tags/dev.trees
gold_train_trees=../sample_data/cp_datasets/PTB_pred_tags/train.trees


###PG###
No_samples=6
pg_variance_reduce=True
variance_reduce_burn_in=999
pg_valsteps=500
entropy_regularisation=True
entropy_reg_coeff=0.01



